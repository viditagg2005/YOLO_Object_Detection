{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Finetuning DINO","metadata":{}},{"cell_type":"markdown","source":"## Cloning the DINO repository and install dependencies","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/IDEA-Research/DINO.git /kaggle/working/DINO\n%cd /kaggle/working/DINO\n!pip install -r requirements.txt\n!pip install -e .\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Imports and Dataset Definition","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as T\nfrom PIL import Image\n\nclass YoloTrafficDataset(Dataset):\n    def __init__(self, root, img_size=800, transforms=None):\n        \"\"\"\n        Expects:\n          root/\n            images/*.jpg\n            labels/*.txt  # YOLO format: class x_center y_center width height (normalized)\n        \"\"\"\n        self.img_paths = sorted(glob.glob(os.path.join(root, \"images\", \"*.jpg\")))\n        self.label_paths = [\n            p.replace(\"images\", \"labels\").replace(\".jpg\", \".txt\")\n            for p in self.img_paths\n        ]\n        self.img_size = img_size\n        self.transforms = transforms or T.Compose([\n            T.Resize((img_size, img_size)),\n            T.ToTensor(),\n            T.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n        ])\n\n    def __len__(self):\n        return len(self.img_paths)\n\n    def __getitem__(self, idx):\n        img = Image.open(self.img_paths[idx]).convert(\"RGB\")\n        w, h = img.size\n        boxes, labels = [], []\n        with open(self.label_paths[idx], \"r\") as f:\n            for line in f:\n                cls, xc, yc, bw, bh = map(float, line.split())\n                x1 = (xc - bw/2) * w\n                y1 = (yc - bh/2) * h\n                x2 = (xc + bw/2) * w\n                y2 = (yc + bh/2) * h\n                boxes.append([x1, y1, x2, y2])\n                labels.append(int(cls))\n        target = {\n            \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n            \"labels\": torch.tensor(labels, dtype=torch.int64),\n        }\n        img = self.transforms(img)\n        return img, target\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def collate_fn(batch):\n    imgs, targets = zip(*batch)\n    return torch.stack(imgs), list(targets)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"from dino.models import build_model\nfrom dino.config import get_config\n\ndef train_dino(\n    data_root,\n    config_file,\n    pretrained_checkpoint,\n    output_dir,\n    img_size=800,\n    batch_size=4,\n    lr=2e-5,\n    weight_decay=1e-4,\n    lr_step=8,\n    lr_gamma=0.1,\n    epochs=15,\n):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Datasets and loaders\n    train_ds = YoloTrafficDataset(data_root, img_size=img_size)\n    val_ds   = YoloTrafficDataset(data_root, img_size=img_size)\n    train_loader = DataLoader(\n        train_ds, batch_size=batch_size, shuffle=True,\n        collate_fn=collate_fn, num_workers=4\n    )\n    val_loader = DataLoader(\n        val_ds, batch_size=batch_size, shuffle=False,\n        collate_fn=collate_fn, num_workers=4\n    )\n\n    # Build and load DINO\n    cfg = get_config()\n    cfg.merge_from_file(config_file)\n    model, criterion, postprocessors = build_model(cfg)\n    model.to(device)\n    ckpt = torch.load(pretrained_checkpoint, map_location=\"cpu\")\n    model.load_state_dict(ckpt[\"model\"], strict=False)\n\n    # Optimizer and scheduler\n    optimizer = torch.optim.AdamW(\n        model.parameters(), lr=lr, weight_decay=weight_decay\n    )\n    scheduler = torch.optim.lr_scheduler.StepLR(\n        optimizer, step_size=lr_step, gamma=lr_gamma\n    )\n\n    os.makedirs(output_dir, exist_ok=True)\n    for epoch in range(epochs):\n        model.train()\n        for imgs, targets in train_loader:\n            imgs = imgs.to(device)\n            targ = [{k: v.to(device) for k, v in t.items()} for t in targets]\n            outputs = model(imgs, targ)\n            loss = sum(outputs[\"losses\"].values())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        scheduler.step()\n\n        # Optional validation pass\n        model.eval()\n        with torch.no_grad():\n            for imgs, _ in val_loader:\n                _ = model(imgs.to(device))\n\n        torch.save(\n            {\"model\": model.state_dict()},\n            os.path.join(output_dir, f\"checkpoint_{epoch}.pth\")\n        )\n    print(\"Fine-tuning complete.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dino(\n    data_root=\"/kaggle/input/yolo-dataset/dataset_1/dataset_1/data.yaml\", # we have similarly run for dataset part 2 and 3\n    config_file=\"configs/DINO/dino_4scale_12ep_res50.yaml\",\n    pretrained_checkpoint=\"checkpoints/dino_res50_12ep.pth\",\n    output_dir=\"/kaggle/working/\",\n    img_size=800,\n    batch_size=4,\n    lr=2e-5,\n    weight_decay=1e-4,\n    lr_step=8,\n    lr_gamma=0.1,\n    epochs=15,\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}