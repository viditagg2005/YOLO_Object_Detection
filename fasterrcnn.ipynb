{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T13:20:43.928420Z","iopub.execute_input":"2025-05-19T13:20:43.929098Z","iopub.status.idle":"2025-05-19T13:20:43.932534Z","shell.execute_reply.started":"2025-05-19T13:20:43.929073Z","shell.execute_reply":"2025-05-19T13:20:43.931961Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if not os.path.exists(\"transforms.py\"):\n        with open(\"transforms.py\", \"w\") as f:\n            f.write(\"\"\"\nimport random\nimport torch\nfrom torchvision.transforms import functional as F\n\nclass Compose:\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, image, target):\n        for t in self.transforms:\n            image, target = t(image, target)\n        return image, target\n\nclass ToTensor:\n    def __call__(self, image, target):\n        image = F.to_tensor(image)\n        return image, target\n\nclass RandomHorizontalFlip:\n    def __init__(self, prob):\n        self.prob = prob\n\n    def __call__(self, image, target):\n        if random.random() < self.prob:\n            height, width = image.shape[-2:]\n            image = image.flip(-1)\n            bbox = target[\"boxes\"]\n            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]\n            target[\"boxes\"] = bbox\n        return image, target\n\"\"\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T13:20:43.934660Z","iopub.execute_input":"2025-05-19T13:20:43.934989Z","iopub.status.idle":"2025-05-19T13:20:43.951087Z","shell.execute_reply.started":"2025-05-19T13:20:43.934972Z","shell.execute_reply":"2025-05-19T13:20:43.950521Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport torchvision\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nimport transforms as T\nfrom tqdm import tqdm\nfrom torchmetrics.detection.mean_ap import MeanAveragePrecision\nimport random\nfrom sklearn.model_selection import train_test_split\nimport cv2\n\nclass YOLOtoRCNNDataset(Dataset):\n    def __init__(self, img_dir, label_dir, transforms=None, class_names=None):\n        self.img_dir = img_dir\n        self.label_dir = label_dir\n        self.transforms = transforms\n        \n        # Get all image files with annotations\n        self.imgs = [f for f in os.listdir(img_dir) if os.path.exists(os.path.join(label_dir, os.path.splitext(f)[0] + '.txt'))]\n        \n        # Default class names\n        if class_names is None:\n            self.class_names = {\n                0: 'background',  # Required for RCNN\n                1: 'biker',      \n                2: 'car',      \n                3: 'pedestrian',      \n                4: 'traffic-light',\n                5: 'traffic-light-green',\n                6: 'traffic-light-greenleft',\n                7: 'traffic-light-red',\n                8: 'traffic-light-redleft',\n                9: 'traffic-light-yellow',\n                10: 'traffic-light-yellowleft',\n                11: 'truck',\n                12:'arret'\n            }\n        else:\n            self.class_names = class_names\n            \n    def __len__(self):\n        return len(self.imgs)\n    \n    def __getitem__(self, idx):\n        # Load image\n        img_path = os.path.join(self.img_dir, self.imgs[idx])\n        img = Image.open(img_path).convert(\"RGB\")\n        \n        # Load annotation\n        label_path = os.path.join(self.label_dir, os.path.splitext(self.imgs[idx])[0] + '.txt')\n        \n        # Get image dimensions\n        width, height = img.size\n        \n        boxes = []\n        labels = []\n        \n        # Check if file exists and is not empty\n        if os.path.exists(label_path) and os.path.getsize(label_path) > 0:\n            with open(label_path, 'r') as f:\n                for line in f.readlines():\n                    line = line.strip()\n                    if not line:  # Skip empty lines\n                        continue\n                    \n                    data = line.split(' ')\n                    if len(data) < 5:  # Skip invalid lines\n                        continue\n                        \n                    # YOLO format: class_id center_x center_y width height (normalized)\n                    try:\n                        class_id = int(data[0])\n                        x_center = float(data[1]) * width\n                        y_center = float(data[2]) * height\n                        box_width = float(data[3]) * width\n                        box_height = float(data[4]) * height\n                        \n                        # Convert to [x_min, y_min, x_max, y_max] format for RCNN\n                        x_min = x_center - (box_width / 2)\n                        y_min = y_center - (box_height / 2)\n                        x_max = x_center + (box_width / 2)\n                        y_max = y_center + (box_height / 2)\n                        \n                        boxes.append([x_min, y_min, x_max, y_max])\n                        # Add 1 to class_id to account for background class in RCNN\n                        labels.append(class_id + 1)\n                    except (ValueError, IndexError):\n                        # Skip invalid entries\n                        continue\n        \n        # Handle empty annotations\n        if len(boxes) == 0:\n            # Create a dummy box outside the image (will be filtered out during training)\n            boxes = torch.zeros((0, 4), dtype=torch.float32)\n            labels = torch.zeros((0), dtype=torch.int64)\n            area = torch.zeros((0), dtype=torch.float32)\n            iscrowd = torch.zeros((0), dtype=torch.int64)\n        else:\n            # Convert to tensors\n            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n            labels = torch.as_tensor(labels, dtype=torch.int64)\n            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n            iscrowd = torch.zeros((len(boxes),), dtype=torch.int64)\n        \n        image_id = torch.tensor([idx])\n        \n        target = {\n            'boxes': boxes,\n            'labels': labels,\n            'image_id': image_id,\n            'area': area,\n            'iscrowd': iscrowd\n        }\n        \n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n            \n        return img, target\n\n# Transforms for data augmentation and normalization\ndef get_transform(train):\n    transforms = []\n    transforms.append(T.ToTensor())\n    if train:\n        transforms.append(T.RandomHorizontalFlip(0.5))\n    return T.Compose(transforms)\n\n# Function to get the Faster R-CNN model\ndef get_model(num_classes):\n    # Load pre-trained model\n    model = fasterrcnn_resnet50_fpn(weights='DEFAULT')\n    \n    # Replace the classifier with a new one for our number of classes\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n    \n    return model\n\n# Custom collate function to handle empty annotations\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\n# Training function\ndef train_one_epoch(model, optimizer, data_loader, device):\n    model.train()\n    \n    total_loss = 0\n    num_batches = 0\n    \n    for images, targets in tqdm(data_loader):\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n        \n        # Skip batches with only empty annotations\n        if all(len(t[\"boxes\"]) == 0 for t in targets):\n            continue\n            \n        try:\n            loss_dict = model(images, targets)\n            losses = sum(loss for loss in loss_dict.values())\n            \n            optimizer.zero_grad()\n            losses.backward()\n            optimizer.step()\n            \n            total_loss += losses.item()\n            num_batches += 1\n        except Exception as e:\n            print(f\"Error in batch: {e}\")\n            continue\n    \n    if num_batches == 0:\n        return 0.0\n    return total_loss / num_batches\n\n# Evaluation function\ndef evaluate(model, data_loader, device):\n    model.eval()\n    metric = MeanAveragePrecision()\n    \n    with torch.no_grad():\n        for images, targets in tqdm(data_loader):\n            images = list(img.to(device) for img in images)\n            \n            # Skip images with no annotations during evaluation\n            valid_targets = []\n            valid_images = []\n            valid_indices = []\n            \n            for i, target in enumerate(targets):\n                if len(target[\"boxes\"]) > 0:\n                    valid_targets.append({k: v.to(device) for k, v in target.items()})\n                    valid_images.append(images[i])\n                    valid_indices.append(i)\n            \n            if not valid_images:\n                continue\n                \n            try:\n                outputs = model(valid_images)\n                \n                # Format outputs for MeanAveragePrecision\n                preds = []\n                for output in outputs:\n                    pred = {\n                        'boxes': output['boxes'],\n                        'scores': output['scores'],\n                        'labels': output['labels']\n                    }\n                    preds.append(pred)\n                \n                metric.update(preds, valid_targets)\n            except Exception as e:\n                print(f\"Error during evaluation: {e}\")\n                continue\n    \n    return metric.compute()\n\n# Set random seed for reproducibility\ndef set_seed(seed=42):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n# Main function\ndef main():\n    # Set random seed\n    set_seed()\n    \n    # Define paths\n    base_dir = \"/kaggle/input/yolo-dataset/dataset_1/dataset_1/train\"\n    img_dir = os.path.join(base_dir, \"images\")\n    label_dir = os.path.join(base_dir, \"labels\")\n    \n    print(\"Starting RCNN training with YOLO annotations...\")\n    print(\"Note: YOLO class IDs will be shifted by +1 to accommodate RCNN's background class\")\n    \n    # Check if directories exist\n    if not os.path.exists(img_dir) or not os.path.exists(label_dir):\n        print(f\"Error: Directories not found - images: {os.path.exists(img_dir)}, labels: {os.path.exists(label_dir)}\")\n        return\n    \n    # Automatically determine the class IDs from your YOLO annotations\n    print(\"Scanning labels to determine classes...\")\n    unique_class_ids = set()\n\n    for i in range(12):\n      unique_class_ids.add(i)\n    \n    \n    print(f\"Found {len(unique_class_ids)} unique classes in YOLO annotations: {sorted(unique_class_ids)}\")\n    \n    # Define class names with background as 0 and YOLO classes shifted by +1\n    class_names = {0: 'background'} \n    \n    # Add the YOLO classes with +1 offset\n    for yolo_class_id in sorted(unique_class_ids):\n        rcnn_class_id = yolo_class_id + 1\n        class_names[rcnn_class_id] = f'class{yolo_class_id}'\n    \n    print(f\"Mapped to RCNN classes: {class_names}\")\n    \n    \n    # Number of classes (including background)\n    num_classes = len(class_names)\n    \n    # Create dataset\n    full_dataset = YOLOtoRCNNDataset(img_dir, label_dir, get_transform(train=True), class_names)\n    \n    # Filter out empty annotations for training\n    valid_indices = []\n    print(\"Checking for valid annotations...\")\n    for idx in tqdm(range(len(full_dataset))):\n        _, target = full_dataset[idx]\n        if len(target[\"boxes\"]) > 0:\n            valid_indices.append(idx)\n    \n    print(f\"Found {len(valid_indices)}/{len(full_dataset)} images with valid annotations\")\n    \n    # Split dataset into train and test\n    train_indices, val_indices = train_test_split(valid_indices, test_size=0.2, random_state=42)\n    \n    print(f\"Training on {len(train_indices)} images, validating on {len(val_indices)} images\")\n    \n    # Create data samplers\n    train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n    val_sampler = torch.utils.data.SubsetRandomSampler(val_indices)\n    \n    # Create data loaders with custom collate function\n    train_loader = DataLoader(\n        full_dataset,\n        batch_size=4,\n        sampler=train_sampler,\n        collate_fn=collate_fn\n    )\n    \n    val_dataset = YOLOtoRCNNDataset(img_dir, label_dir, get_transform(train=False), class_names)\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=2,\n        sampler=val_sampler,\n        collate_fn=collate_fn\n    )\n    \n    # Get device\n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    print(f\"Using device: {device}\")\n    \n    # Get model\n    model = get_model(num_classes)\n    model.to(device)\n    \n    # Define optimizer\n    params = [p for p in model.parameters() if p.requires_grad]\n    optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n    \n    # Learning rate scheduler\n    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n    \n    # Number of epochs\n    num_epochs = 1\n    \n    # Training loop\n    print(\"Starting training...\")\n    for epoch in range(num_epochs):\n        # Train for one epoch\n        train_loss = train_one_epoch(model, optimizer, train_loader, device)\n        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}\")\n        \n        # Update the learning rate\n        lr_scheduler.step()\n        \n        # Evaluate on the validation dataset\n        if (epoch + 1) % 2 == 0 or (epoch + 1) == num_epochs:\n            print(\"Evaluating...\")\n            metrics = evaluate(model, val_loader, device)\n            print(f\"Validation mAP: {metrics['map']:.4f}\")\n            print(f\"mAP@50: {metrics['map_50']:.4f}\")\n            print(f\"mAP@75: {metrics['map_75']:.4f}\")\n    \n    # Save the model\n    torch.save(model.state_dict(), \"faster_rcnn_model.pth\")\n    print(\"Model saved to faster_rcnn_model.pth\")\n    \n    # Final evaluation\n    print(\"Performing final evaluation...\")\n    final_metrics = evaluate(model, val_loader, device)\n    print(\"Final Evaluation Metrics:\")\n    for k, v in final_metrics.items():\n        print(f\"{k}: {v:.4f}\")\n\n    # Visualize a few predictions\n    visualize_predictions(model, val_dataset, val_indices[:5], device, class_names)\n\ndef visualize_predictions(model, dataset, indices, device, class_names):\n    model.eval()\n    \n    # Make sure we have indices to visualize\n    if not indices:\n        print(\"No valid images to visualize\")\n        return\n        \n    fig, axs = plt.subplots(len(indices), 2, figsize=(15, 5 * len(indices)))\n    \n    # Handle single image case\n    if len(indices) == 1:\n        axs = np.array([axs])\n    \n    for i, idx in enumerate(indices):\n        img, target = dataset[idx]\n        # Original image with ground truth\n        img_np = img.permute(1, 2, 0).cpu().numpy()\n        \n        # Handle single image case\n        if len(indices) == 1:\n            ax_gt = axs[0]\n            ax_pred = axs[1]\n        else:\n            ax_gt = axs[i, 0]\n            ax_pred = axs[i, 1]\n            \n        ax_gt.imshow(img_np)\n        ax_gt.set_title('Ground Truth')\n        \n        # Draw ground truth boxes\n        if len(target['boxes']) > 0:\n            for box, label in zip(target['boxes'], target['labels']):\n                box = box.cpu().numpy()\n                rect = plt.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1],\n                                   linewidth=2, edgecolor='r', facecolor='none')\n                ax_gt.add_patch(rect)\n                ax_gt.text(box[0], box[1], class_names[label.item()], \n                             color='white', fontsize=10, bbox=dict(facecolor='r', alpha=0.5))\n        else:\n            ax_gt.text(10, 10, \"No annotations\", color='red', fontsize=12)\n        \n        # Prediction\n        ax_pred.imshow(img_np)\n        ax_pred.set_title('Prediction')\n        \n        with torch.no_grad():\n            try:\n                prediction = model([img.to(device)])\n                prediction = {k: v.cpu() for k, v in prediction[0].items()}\n                \n                if len(prediction['boxes']) > 0:\n                    for box, score, label in zip(prediction['boxes'], prediction['scores'], prediction['labels']):\n                        if score > 0.5:  # Only show predictions with confidence > 0.5\n                            box = box.numpy()\n                            rect = plt.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1],\n                                               linewidth=2, edgecolor='b', facecolor='none')\n                            ax_pred.add_patch(rect)\n                            ax_pred.text(box[0], box[1], f\"{class_names[label.item()]}: {score:.2f}\", \n                                         color='white', fontsize=10, bbox=dict(facecolor='b', alpha=0.5))\n                else:\n                    ax_pred.text(10, 10, \"No predictions\", color='blue', fontsize=12)\n            except Exception as e:\n                print(f\"Error visualizing predictions for image {idx}: {e}\")\n                ax_pred.text(10, 10, \"Error in prediction\", color='red', fontsize=12)\n    \n    plt.tight_layout()\n    plt.savefig('predictions.png')\n    plt.close()\n    print(\"Predictions visualization saved to predictions.png\")\n\nif __name__ == \"__main__\":\n    # Create transforms.py file if it doesn't exist\n    if not os.path.exists(\"transforms.py\"):\n        with open(\"transforms.py\", \"w\") as f:\n            f.write(\"\"\"\nimport random\nimport torch\nfrom torchvision.transforms import functional as F\n\nclass Compose:\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, image, target):\n        for t in self.transforms:\n            image, target = t(image, target)\n        return image, target\n\nclass ToTensor:\n    def __call__(self, image, target):\n        image = F.to_tensor(image)\n        return image, target\n\nclass RandomHorizontalFlip:\n    def __init__(self, prob):\n        self.prob = prob\n\n    def __call__(self, image, target):\n        if random.random() < self.prob:\n            height, width = image.shape[-2:]\n            image = image.flip(-1)\n            bbox = target[\"boxes\"]\n            # Only flip if there are boxes\n            if len(bbox) > 0:\n                bbox[:, [0, 2]] = width - bbox[:, [2, 0]]\n                target[\"boxes\"] = bbox\n        return image, target\n\"\"\")\n    \n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T13:20:44.147579Z","iopub.execute_input":"2025-05-19T13:20:44.148146Z","execution_failed":"2025-05-19T13:22:14.234Z"}},"outputs":[],"execution_count":null}]}